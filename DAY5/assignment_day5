import requests
from bs4 import BeautifulSoup
import concurrent.futures

file = "./new_file.txt"

def get_links(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    links = set()
    for link in soup.find_all('a', href = True):
        href = link['href']
        if href.startswith(('http://', 'https://')):
            links.add(href)
        elif href.startswith('/'):
            absolute_url = requests.compat.urljoin(url, href)
            links.add(absolute_url)
    return list(links)

def write_links(links, filename):
    with open(filename, 'w') as f:
        for link in links:
            f.write(link + '\n')
    print(f"Successfully wrote {len(links)} links to {filename}")
    
def process(url):
    return get_links(url)

if __name__ == "__main__":
    target = "https://www.google.com"

    with concurrent.futures.ThreadPoolExecutor() as pool:
        future = pool.submit(process, target)
        result = future.result()
        write_links(result, file)
